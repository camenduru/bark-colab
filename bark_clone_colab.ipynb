{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/bark-colab/blob/main/bark_clone_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngr4pZeHCM1C"
      },
      "outputs": [],
      "source": [
        "# https://github.com/serp-ai/bark-with-voice-clone/blob/main/clone_voice.ipynb modified\n",
        "# !pip install -q git+https://github.com/camenduru/bark.git\n",
        "!git clone https://github.com/camenduru/bark\n",
        "%cd /content/bark\n",
        "!pip install .\n",
        "\n",
        "!pip install -q --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
        "!pip install -q gradio==3.27.0\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/bark_v0/resolve/main/6285677e88715abde42a9924db939b3b.pt -d /root/.cache/suno/bark_v0 -o 6285677e88715abde42a9924db939b3b.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/bark_v0/resolve/main/751d4d3d562e63ead5311ebe2a5f45a8.pt -d /root/.cache/suno/bark_v0 -o 751d4d3d562e63ead5311ebe2a5f45a8.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/bark_v0/resolve/main/e32937063d7ccececc61b2d2a3bb0a13.pt -d /root/.cache/suno/bark_v0 -o e32937063d7ccececc61b2d2a3bb0a13.pt\n",
        "\n",
        "from bark.generation import load_codec_model, generate_text_semantic\n",
        "from encodec.utils import convert_audio\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "model = load_codec_model(use_gpu=True)\n",
        "\n",
        "# Load and pre-process the audio waveform\n",
        "audio_filepath = '/content/audio.wav' # the audio you want to clone (will get truncated so 5-10 seconds is probably fine, existing samples that I checked are around 7 seconds)\n",
        "device = 'cuda' # or 'cpu'\n",
        "wav, sr = torchaudio.load(audio_filepath)\n",
        "wav = convert_audio(wav, sr, model.sample_rate, model.channels)\n",
        "wav = wav.unsqueeze(0).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract discrete codes from EnCodec\n",
        "with torch.no_grad():\n",
        "    encoded_frames = model.encode(wav)\n",
        "codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Transcription of the audio you are cloning\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get seconds of audio\n",
        "seconds = wav.shape[-1] / model.sample_rate\n",
        "# generate semantic tokens\n",
        "semantic_tokens = generate_text_semantic(text, max_gen_duration_s=seconds, top_k=50, top_p=.95, temp=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# move codes to cpu\n",
        "codes = codes.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "voice_name = 'output' # whatever you want the name of the voice to be\n",
        "output_path = 'bark/assets/prompts/' + voice_name + '.npz'\n",
        "np.savez(output_path, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
